{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7342dec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Similarity Scores: [192  33]\n",
      "Attention Weights: [1.00000000e+00 8.85477188e-70]\n",
      "Final Model Context: [-10.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each set of scores in x.\"\"\"\n",
    "    e_x = np.exp(x - np.max(x)) # Subtract max for numerical stability\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "# 1. Setup the Vectors [Volatility, Trend, Panic]\n",
    "query_today = np.array([10, -5, 8])   # High Stress\n",
    "key_2008    = np.array([ 9, -6, 9])   # High Stress (Match)\n",
    "key_2017    = np.array([ 2, -1, 1])   # Low Stress (Mismatch)\n",
    "\n",
    "# Values: What actually happened next? (e.g., next day return)\n",
    "value_2008  = np.array([-10.0]) # Crash continued\n",
    "value_2017  = np.array([  5.0]) # Rebound\n",
    "\n",
    "# 2. Step A: Calculate Raw Attention Scores (Dot Product)\n",
    "# How similar is Today to 2008?\n",
    "score_2008 = np.dot(query_today, key_2008) \n",
    "# How similar is Today to 2017?\n",
    "score_2017 = np.dot(query_today, key_2017)\n",
    "\n",
    "scores = np.array([score_2008, score_2017])\n",
    "print(f\"Raw Similarity Scores: {scores}\") \n",
    "# Output approx: [192, 33]\n",
    "\n",
    "# 3. Step B: Normalize to Weights (Softmax)\n",
    "weights = softmax(scores)\n",
    "print(f\"Attention Weights: {weights}\")\n",
    "# Output approx: [0.999..., 0.000...] (Almost 100% focus on 2008)\n",
    "\n",
    "# 4. Step C: Calculate Context (Weighted Sum)\n",
    "# This is the 'Fusion' vector passed to the final prediction layer\n",
    "context = (weights[0] * value_2008) + (weights[1] * value_2017)\n",
    "\n",
    "print(f\"Final Model Context: {context}\")\n",
    "# Output: -9.99... (The model effectively ignores the 2017 rebound data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
